from __future__ import annotations

"""Graph generation utilities."""

import logging
import math
import random
from typing import Dict, Iterable

import networkx as nx
import numpy as np


def generate_graph_rnn_like(num_nodes: int, num_edges: int) -> nx.Graph:
    """Return a random graph mimicking GraphRNN output.

    This simplified implementation generates an undirected graph with
    ``num_nodes`` nodes and roughly ``num_edges`` edges using a uniform
    random model. It serves as a lightweight stand-in for a true
    GraphRNN model.
    """
    g = nx.gnm_random_graph(num_nodes, num_edges, seed=0)
    return g


def generate_graph_rnn(
    num_nodes: int,
    num_edges: int,
    p: float = 0.5,
    *,
    directed: bool = False,
) -> nx.Graph:
    """Return a graph built sequentially in the spirit of GraphRNN.

    Nodes are added one by one and connected to previous nodes with
    probability ``p`` until ``num_edges`` edges exist.  This is not a full
    GraphRNN implementation but provides a closer approximation than
    :func:`generate_graph_rnn_like`.

    Parameters
    ----------
    num_nodes:
        Total number of nodes in the generated graph.
    num_edges:
        Desired number of edges.  The algorithm stops once this many
        edges have been added or no more edges can be created.
    p:
        Probability of connecting a new node to any previous node.
    directed:
        If ``True``, create a :class:`~networkx.DiGraph` with edges from
        newer nodes toward older ones.

    Returns
    -------
    networkx.Graph
        The generated graph.
    """

    g = nx.DiGraph() if directed else nx.Graph()
    for i in range(num_nodes):
        g.add_node(i)
        for j in range(i):
            if g.number_of_edges() >= num_edges:
                break
            if random.random() < p:
                g.add_edge(i, j)

    # If we didn't reach ``num_edges`` try to add random edges
    remaining = num_edges - g.number_of_edges()
    if remaining > 0:
        possible = list(nx.non_edges(g))
        random.shuffle(possible)
        for u, v in possible[:remaining]:
            g.add_edge(u, v)
            if g.number_of_edges() >= num_edges:
                break

    return g


def generate_graph_rnn_stateful(
    num_nodes: int,
    num_edges: int,
    *,
    hidden_dim: int = 8,
    seed: int | None = None,
) -> nx.DiGraph:
    """Return a directed graph using a tiny RNN-style process.

    Edges are added sequentially between new nodes and previous ones. The
    connection probability is governed by a simple recurrent update without
    requiring heavy deep-learning dependencies. This serves as a closer
    approximation of GraphRNN than :func:`generate_graph_rnn` while remaining
    lightweight for tests.

    Parameters
    ----------
    num_nodes:
        Number of nodes in the graph.
    num_edges:
        Maximum number of edges to add.
    hidden_dim:
        Dimension of the internal state vector controlling edge probabilities.
    seed:
        Optional seed for reproducibility.

    Returns
    -------
    nx.DiGraph
        Directed graph generated by the process.
    """

    rng = random.Random(seed)
    rs = np.random.default_rng(seed)

    W_in = rs.normal(size=(hidden_dim, 1))
    W_h = rs.normal(size=(hidden_dim, hidden_dim))
    b = rs.normal(size=(hidden_dim,))
    V = rs.normal(size=(hidden_dim,))

    g = nx.DiGraph()
    g.add_node(0)

    for i in range(1, num_nodes):
        g.add_node(i)
        h = np.tanh(rs.normal(size=(hidden_dim,)))
        prev = 0.0
        for j in range(i):
            if g.number_of_edges() >= num_edges:
                break
            h = np.tanh(W_h @ h + W_in[:, 0] * prev + b)
            z = float(V @ h)
            p = 1.0 / (1.0 + math.exp(-z))
            if rng.random() < p:
                g.add_edge(i, j)
                prev = 1.0
            else:
                prev = 0.0

    return g


def generate_graph_rnn_sequential(
    num_nodes: int,
    num_edges: int,
    *,
    hidden_dim: int = 8,
    seed: int | None = None,
    directed: bool = True,
) -> nx.Graph:
    """Return a graph using a sequential RNN to model edge probabilities.

    Parameters
    ----------
    num_nodes:
        Number of nodes in the graph.
    num_edges:
        Maximum number of edges to add.
    hidden_dim:
        Dimension of the internal recurrent state.
    seed:
        Optional random seed for reproducibility.
    directed:
        When ``True`` (default) produce a :class:`~networkx.DiGraph` with edges
        from newer nodes toward older ones. If ``False``, an undirected graph is
        returned.
    """

    rng = random.Random(seed)
    rs = np.random.default_rng(seed)

    W_in = rs.normal(size=(hidden_dim, 1))
    W_h = rs.normal(size=(hidden_dim, hidden_dim))
    b = rs.normal(size=(hidden_dim,))
    V = rs.normal(size=(hidden_dim,))

    g = nx.DiGraph() if directed else nx.Graph()
    g.add_node(0)
    for i in range(1, num_nodes):
        g.add_node(i)
        h = np.tanh(rs.normal(size=(hidden_dim,)))
        prev = 0.0
        for j in range(i):
            if g.number_of_edges() >= num_edges:
                break
            h = np.tanh(W_h @ h + W_in[:, 0] * prev + b)
            z = float(V @ h)
            p = 1.0 / (1.0 + math.exp(-z))
            if rng.random() < p:
                g.add_edge(i, j)
                prev = 1.0
            else:
                prev = 0.0
    return g


def generate_netgan_like(
    g: nx.Graph,
    num_walks: int = 32,
    walk_length: int = 16,
    p: float = 0.5,
) -> nx.Graph:
    """Return a graph grown from random walks mimicking NetGAN.

    This lightweight implementation performs a series of random walks on the
    input graph ``g`` to sample plausible edge pairs.  Edges from the walks are
    reconnected in a new graph with probability ``p`` in order to approximate the
    distribution learned by a true NetGAN model.

    Parameters
    ----------
    g:
        Reference graph used to sample random walks.
    num_walks:
        Number of random walks to generate.
    walk_length:
        Length of each walk.
    p:
        Probability of keeping an edge encountered in a walk.

    Returns
    -------
    nx.Graph
        New graph built from the sampled walks.
    """

    new_g = nx.Graph()
    new_g.add_nodes_from(g.nodes())
    nodes = list(g.nodes())
    if not nodes:
        return new_g

    rng = random.Random(0)
    for _ in range(num_walks):
        current = rng.choice(nodes)
        for _ in range(walk_length - 1):
            neighbors = list(g.neighbors(current))
            if not neighbors:
                break
            nxt = rng.choice(neighbors)
            if rng.random() < p and not new_g.has_edge(current, nxt):
                new_g.add_edge(current, nxt)
            current = nxt

    return new_g


def sheaf_consistency_real(
    graph: nx.Graph, b: Iterable[float], *, edge_attr: str = "sheaf_sign"
) -> float:
    """Return sheaf consistency score solving \Delta_F x = b.

    Parameters
    ----------
    graph:
        Input graph carrying sheaf restrictions in ``edge_attr``.
    b:
        Constraint vector for nodes ordered as in ``graph.nodes``.
    edge_attr:
        Name of the edge attribute storing restriction signs.

    Returns
    -------
    float
        Score ``1/(1 + ||b - \Delta x||_2)`` after a least-squares solve.
    """
    import numpy as np
    from scipy.sparse import csr_matrix
    from scipy.sparse.linalg import cg

    from .sheaf import sheaf_laplacian

    L = sheaf_laplacian(graph, edge_attr=edge_attr)
    if L.size == 0:
        return 1.0
    b_vec = np.asarray(list(b), dtype=float)
    A = csr_matrix(L)
    x, _ = cg(A, b_vec, atol=1e-6)
    resid = b_vec - A @ x
    return 1.0 / (1.0 + float(np.linalg.norm(resid)))


def bias_reweighting(
    neighbors_demog: Dict[str, int],
    global_demog: Dict[str, int],
    weights: Dict[str, float],
    *,
    threshold: float = 0.1,
) -> Dict[str, float]:
    """Return reweighted sampling probabilities based on demographics.

    The Wasserstein distance between the local neighbor distribution and the
    global demographic distribution is computed. If it exceeds ``threshold`` the
    weights of under-represented groups are upweighted by 20%.
    """
    import numpy as np
    from scipy.stats import wasserstein_distance

    if not neighbors_demog:
        return weights
    keys = sorted(set(global_demog) | set(neighbors_demog))
    local = np.array([neighbors_demog.get(k, 0) for k in keys], dtype=float)
    global_ = np.array([global_demog.get(k, 0) for k in keys], dtype=float)
    if global_.sum() == 0:
        return weights
    local /= max(local.sum(), 1.0)
    global_ /= global_.sum()
    w_dist = wasserstein_distance(local, global_)
    if w_dist <= threshold:
        return weights

    adjusted = dict(weights)
    for k in keys:
        if local[keys.index(k)] < global_[keys.index(k)]:
            adjusted[k] = weights.get(k, 0.0) * 1.2
    return adjusted


def sheaf_score(b: Iterable[float], Delta) -> float:
    """Return real sheaf consistency ``1/(1 + ||b - \Delta x||_2)``.

    Parameters
    ----------
    b:
        Right-hand side vector.
    Delta:
        Sheaf Laplacian matrix ``\Delta``.
    """

    import numpy as np
    from scipy.sparse.linalg import cg

    b_vec = np.asarray(list(b), dtype=float)
    x, _ = cg(Delta, b_vec, atol=0.0, rtol=1e-5, maxiter=1000)
    resid = b_vec - Delta @ x
    score = 1.0 / (1.0 + np.linalg.norm(resid))
    try:
        from .monitoring import sheaf_score_g as _sheaf_score_gauge

        if _sheaf_score_gauge is not None:
            _sheaf_score_gauge.set(float(score))
    except Exception:
        pass
    return score


def bias_wasserstein(loc_hist, glob_hist, logits):
    """Return logits scaled by ``exp(-W)`` where ``W`` is the Wasserstein distance."""

    import numpy as np
    import torch
    from geomloss import SamplesLoss

    loss = SamplesLoss("sinkhorn", blur=0.01)
    loc_t = torch.as_tensor(loc_hist, dtype=torch.float32)
    glob_t = torch.as_tensor(glob_hist, dtype=torch.float32)
    W = float(loss(loc_t, glob_t))
    beta = float(np.exp(-W))
    scaled = np.array(logits, dtype=float, copy=True)
    if W > 0.1:
        scaled *= beta
    try:
        from .monitoring import bias_wasserstein_last as _bw_gauge

        if _bw_gauge is not None:
            _bw_gauge.set(W)
    except Exception:  # pragma: no cover - optional metrics
        pass
    return scaled, W


def apply_logit_bias(payload, loc_hist, glob_hist):
    """Apply Wasserstein bias mitigation to LLM ``payload`` logits.

    Parameters
    ----------
    payload:
        Dictionary containing at least a ``"logits"`` entry.
    loc_hist:
        Local histogram of observed classes.
    glob_hist:
        Global reference histogram.

    Returns
    -------
    float
        The Wasserstein distance ``W`` between ``loc_hist`` and ``glob_hist``.
    """

    import numpy as np

    if "logits" not in payload:
        raise KeyError("payload missing 'logits'")

    scaled_logits, W = bias_wasserstein(loc_hist, glob_hist, payload["logits"])
    payload["logits"] = np.asarray(scaled_logits, dtype=float).tolist()
    beta = float(np.exp(-W)) if W > 0.1 else 1.0
    logging.getLogger(__name__).info(
        "Bias factor Î²=%.3f applied; W=%.4f", beta, float(W)
    )
    return W


# -- Format generation helpers --


def generate_chatml(payload, loc_hist, glob_hist):
    """Return ChatML payload with logits scaled by :func:`bias_wasserstein`."""
    scaled_logits, _W = bias_wasserstein(loc_hist, glob_hist, payload["logits"])
    payload["logits"] = list(scaled_logits)
    return payload


def generate_alpaca(payload, loc_hist, glob_hist):
    """Return Alpaca payload with logits scaled by :func:`bias_wasserstein`."""
    scaled_logits, _W = bias_wasserstein(loc_hist, glob_hist, payload["logits"])
    payload["logits"] = list(scaled_logits)
    return payload
