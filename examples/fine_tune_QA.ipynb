{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune QA example\n",
        "This notebook demonstrates how to fine-tune a question answering model using the `datacreek` toolkit. The workflow covers dataset ingestion with the semantic safety filter, training with Hugging Face `Trainer`, and running inference on the fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# In a Colab environment this cell installs required packages.\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install -q datasets transformers accelerate datacreek\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Load the SQuAD dataset and apply the semantic safety filter.\n\n",
        "The safety filter uses a tiny toxicity model and NSFW regex heuristics to drop unsafe samples before training.\n\n",
        "Variables\n",
        "    dataset: Raw SQuAD dataset split dictionary\n",
        "    safe_dataset: Dataset after filtering\n\"\"\"\n",
        "from datasets import load_dataset\n",
        "from ingest.safety_filter import SafetyFilter\n",
        "\n",
        "dataset = load_dataset('squad')\n",
        "filter = SafetyFilter()\n",
        "\n",
        "def is_safe(example):\n",
        "    text = example['question'] + ' ' + example['context']\n",
        "    return filter(text)\n",
        "\n",
        "safe_dataset = dataset.filter(is_safe)\n",
        "safe_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Tokenize the dataset for the QA task.\n\n",
        "This cell leverages the pretrained tokenizer associated with the chosen model.\n",
        "Variables\n",
        "    tokenizer: Hugging Face tokenizer\n",
        "    tokenized: tokenized dataset ready for training\n\"\"\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess(example):\n",
        "    return tokenizer(example['question'], example['context'], truncation=True)\n",
        "\n",
        "tokenized = safe_dataset.map(preprocess, batched=True)\n",
        "tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Fine-tune the model using Hugging Face Trainer.\n\n",
        "TrainingArguments set a small number of epochs for demonstration. On Colab with a GPU this cell completes within minutes.\n",
        "Variables\n",
        "    model: pretrained transformer model for QA\n",
        "    trainer: Hugging Face Trainer instance\n\"\"\"\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "args = TrainingArguments(\n",
        "    output_dir='qa-model',\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=tokenized['train'])\n",
        "trainer.train()\n",
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Run inference with the fine-tuned model.\n\n",
        "Given a question and context from the validation set, the model predicts the answer span.\n\n",
        "Variables\n",
        "    question: sample question from dataset\n",
        "    context: corresponding context passage\n",
        "    answer: text span predicted by the model\n\"\"\"\n",
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline('question-answering', model='qa-model')\n",
        "sample = safe_dataset['validation'][0]\n",
        "question = sample['question']\n",
        "context = sample['context']\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "answer\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}